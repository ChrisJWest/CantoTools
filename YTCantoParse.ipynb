{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**YouTube Cantonese Subtitle Parsing Metrics**\n",
        "\n",
        "Welcome to YTCantoParse! I built this mostly for fun, but if it helps you at all in your learning I will be so glad!\n",
        "\n",
        "I've split the Colab into two sections. The first section is if you would simply like to analyze a single video. This is not integrated with the spreadsheet.\n",
        "\n",
        "The second section is for recommending you new videos based off of either the known/unknown words or frequency analysis.\n",
        "\n",
        "Both sections will have optional parameters depending on if you use Migaku or not. If there are any issues with the Colab or you have questions, please contact chrisjwest99@gmail.com"
      ],
      "metadata": {
        "id": "Rbtw7K-nJPn1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqgFqsBmJNeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "f5f7dea8-d3a8-461c-e07a-97cd082b01f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WF2FkidAVMLKjdtOf0lWb21d2mYa7-06\n",
            "To: /content/parser.csv\n",
            "100% 415k/415k [00:00<00:00, 121MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PWglfvAwhN9CDBjJiqPt_DkZr132bFAE\n",
            "To: /content/freq.json\n",
            "100% 361k/361k [00:00<00:00, 95.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=151b1fyxMtodhsy2EIgQPUjEo3QyVXoCC\n",
            "To: /content/learning.json\n",
            "100% 157k/157k [00:00<00:00, 96.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "#@title Option A) - Analyze a Single Video\n",
        "#@markdown Use this section if you would like to analyze a Youtube video that you already have a link for.\n",
        "\n",
        "#@markdown By default, we assume you simply want basic frequency analysis with a standard parser and without Migaku learning status integration. If you want to use your own parser or frequency list, it's a bit more difficult and you should see the FAQ at the bottom.\n",
        "\n",
        "#@markdown To use Migaku learning status, it is quite simple; first, check the UseMigaku box below and run the cell. Then, download your Yue wordlist from Migaku settings and rename it as \"learning.json\". Then, upload your file into the Colab filesystem under the \"Files\" tab on the left and replace the current \"learning.json\" file.\n",
        "UseMigaku = True #@param {type:\"boolean\"}\n",
        "\n",
        "### Downloading files for parsing ###\n",
        "\n",
        "!gdown --id 1WF2FkidAVMLKjdtOf0lWb21d2mYa7-06\n",
        "!gdown --id 1PWglfvAwhN9CDBjJiqPt_DkZr132bFAE\n",
        "\n",
        "if (UseMigaku):\n",
        "  !gdown --id 151b1fyxMtodhsy2EIgQPUjEo3QyVXoCC\n",
        "\n",
        "#ParserPath = '' #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this code to complete set-up\n",
        "\n",
        "import csv\n",
        "\n",
        "### Import the Parsing Data ###\n",
        "\n",
        "parser_path = \"/content/parser.csv\"\n",
        "with open(parser_path, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "# We convert to a set for fast-lookup time\n",
        "data_list = []\n",
        "for i in data:\n",
        "  data_list.append(i[0])\n",
        "setParser = set(data_list)\n",
        "\n",
        "\n",
        "### Define our Longest-Substring-Match Parser ###\n",
        "# This uses a simple double-loop structure. Takes a long time\n",
        "# for longer videos but is usually fast-enough and\n",
        "# is very simple.\n",
        "def parseInput(inputString):\n",
        "  parsedOutput = []\n",
        "  i = 0\n",
        "  while (i<len(inputString)):\n",
        "    longestSub = i+1\n",
        "    for ii in range(i+1,i+21):\n",
        "      if (inputString[i:ii] in setParser):\n",
        "        longestSub = ii\n",
        "    parsedOutput.append(inputString[i:longestSub])\n",
        "    i = longestSub\n",
        "  return parsedOutput\n",
        "\n",
        "\n",
        "### Import the Frequency Data ###\n",
        "\n",
        "import json\n",
        "\n",
        "freq_path = \"/content/freq.json\"\n",
        "\n",
        "# Opening JSON file\n",
        "with open(freq_path) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    numList = list(range(len(data)))\n",
        "    freq_dict = dict(zip(data, numList))\n",
        "\n",
        "\n",
        "### OPTIONAL: Import Migaku Learning Status ###\n",
        "# We use a dictionary here for fast-lookup\n",
        "\n",
        "words_path = \"/content/learning.json\"\n",
        "\n",
        "with open(words_path) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    wordList = []\n",
        "    knownList = []\n",
        "    for i in range(len(data)):\n",
        "      word = data[i][0]\n",
        "      wordOnly = word.split(\"◴\")[0]\n",
        "      wordList.append(wordOnly)\n",
        "      knownList.append(data[i][1])\n",
        "    word_dict = dict(zip(wordList, knownList))\n",
        "\n",
        "### Importing Youtube extension for grabbing subtitles ###\n",
        "!pip install youtube-transcript-api\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "print(\"Success\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "MM67wcVfbkUM",
        "outputId": "1339f2a4-132f-450e-b071-867ee5c1bd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from youtube-transcript-api) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->youtube-transcript-api) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->youtube-transcript-api) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->youtube-transcript-api) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->youtube-transcript-api) (1.24.3)\n",
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Paste in the youtube video URL and run this cell to analyze\n",
        "\n",
        "VideoPath = 'https://youtube.com/watch?v=uiN8jfEtEN4' #@param {type:\"string\"}\n",
        "video_id = VideoPath.split(\"v=\")[1]\n",
        "video_id = video_id.split(\"&\")[0]\n",
        "\n",
        "### OLD WAY TO FIND SUBS - PLEASE IGNORE ###\n",
        "\n",
        "### NOTE: This is not ideal but I use a brute force way to\n",
        "#         get past the fact that videos could have any number\n",
        "#         of language codes. This can fail if it uses a special code.\n",
        "#customLanguageCode = \"zh-HK\"\n",
        "\n",
        "#try:\n",
        "#  srt = YouTubeTranscriptApi.get_transcript(video_id, languages=['zh-HK'])\n",
        "#except:\n",
        "#  try:\n",
        "#    srt = YouTubeTranscriptApi.get_transcript(video_id, languages=['yue-HK'])\n",
        "#  except:\n",
        "#      try:\n",
        "#        srt = YouTubeTranscriptApi.get_transcript(video_id, languages=['yue'])\n",
        "#      except:\n",
        "#        try:\n",
        "#          srt = YouTubeTranscriptApi.get_transcript(video_id, languages=[customLanguageCode])\n",
        "#        except:\n",
        "#          raise Exception(\"Error in fetching subtitles: Language code for this video not found. Please find language codec for this video (ex. zh-HK) and set under 'customLanguageCode' in code for this cell.\")\n",
        "\n",
        "### NEW WAY TO FIND SUBS ###\n",
        "\n",
        "codecs = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "\n",
        "# Search for correct cc\n",
        "alltext = \"\"\n",
        "for codec in codecs:\n",
        "  srt = YouTubeTranscriptApi.get_transcript(video_id, languages=[codec.language_code])\n",
        "  # Combine all of the subtitles into one string\n",
        "  alltext = \"\"\n",
        "  for iii in range(len(srt)):\n",
        "    alltext += srt[iii]['text']\n",
        "  if (('係' in alltext) or ('佢' in alltext)):\n",
        "    finalCodec = codec\n",
        "\n",
        "\n",
        "# Parse text into tokens\n",
        "parsed = parseInput(alltext)\n",
        "\n",
        "# Counters to track metrics\n",
        "knownTokensNum = 0\n",
        "learningTokensNum = 0\n",
        "unknownTokensNum = 0\n",
        "totalFreqSum = 0\n",
        "unknownAndSeenFreqSum = 0\n",
        "\n",
        "# Filter only those tokens which are in our parser\n",
        "parseFiltered = []\n",
        "for i in range(len(parsed)):\n",
        "  if (parsed[i] in setParser):\n",
        "    parseFiltered.append(parsed[i])\n",
        "\n",
        "maxFreq = len(freq_dict)\n",
        "for i in range(len(parseFiltered)):\n",
        "\n",
        "  # Total Frequency metrics\n",
        "  word = parseFiltered[i]\n",
        "  freq = freq_dict.get(word, maxFreq)\n",
        "  totalFreqSum += freq\n",
        "\n",
        "  # Word learning status metrics\n",
        "  knownStatus = word_dict.get(word, -1)\n",
        "  if (knownStatus == 1):\n",
        "    learningTokensNum+=1\n",
        "  elif (knownStatus == 2):\n",
        "    knownTokensNum+=1\n",
        "  else:\n",
        "    unknownTokensNum+=1\n",
        "\n",
        "  # Learning status frequency metrics\n",
        "  if ((knownStatus == 1) or (knownStatus == -1)):\n",
        "    freq = freq_dict.get(word, maxFreq)\n",
        "    unknownAndSeenFreqSum += freq\n",
        "\n",
        "totalTokens = learningTokensNum+knownTokensNum+unknownTokensNum\n",
        "ulNum = learningTokensNum+unknownTokensNum\n",
        "\n",
        "print(\"Average Word Frequency: \" + str(round(totalFreqSum / (len(parseFiltered)), 3)))\n",
        "\n",
        "if UseMigaku:\n",
        "  print(\"Percentage known words: \" + str(round(knownTokensNum/totalTokens, 3)))\n",
        "  print(\"Percentage learning words: \" + str(round(learningTokensNum/totalTokens,3)))\n",
        "  print(\"Percentage unknown words: \" + str(round(unknownTokensNum/totalTokens, 3)))\n",
        "  print(\"Average Word Frequency for Learning or Unknown Words: \" + str(round(unknownAndSeenFreqSum/ulNum, 3)))\n"
      ],
      "metadata": {
        "id": "reYdanxTSWz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "8d1783af-c90d-4f95-8184-ed02c985c46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Word Frequency: 2310.526\n",
            "Percentage known words: 0.559\n",
            "Percentage learning words: 0.289\n",
            "Percentage unknown words: 0.152\n",
            "Average Word Frequency for Learning or Unknown Words: 4821.193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Option B) - Recommend a video from Youtube\n",
        "#@markdown Use this section if you would like to be recommended a video from Youtube based on your learning preferences.\n",
        "\n",
        "#@markdown Specifically, I use rtveitch's Cantonese spreadsheet, which can be found here:\n",
        "\n",
        "#@markdown https://docs.google.com/spreadsheets/d/1CmN8GPalrb45YFIPrWgh7GRYyoUhnizEOImY6kAW82w\n",
        "\n",
        "#@markdown Same as in Option A), you can specify if you would like to use Migaku or not when calculating metrics. To use Migaku learning status, it is quite simple; first, check the UseMigaku box below and set your other options and then run the cell. Then, download your Yue wordlist from Migaku settings and rename it as \"learning.json\". Then, upload your file into the Colab filesystem under the \"Files\" tab on the left and replace the current \"learning.json\" file.\n",
        "\n",
        "UseMigaku = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Below are the preferences for how videos are selected for you. Since the spreadsheet has 5000ish Canto Subbed videos, we use a sampling algorithm to randomly select videos from the sheet to sort for metrics. This also hopefully means you aren't always recommended the same videos every time. Also, Youtube has a limit on the number of times I can access it's API every second so this helps stay below that limit.\n",
        "\n",
        "SampleSize = 10 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "\n",
        "!gdown --id 1WF2FkidAVMLKjdtOf0lWb21d2mYa7-06\n",
        "!gdown --id 1PWglfvAwhN9CDBjJiqPt_DkZr132bFAE\n",
        "!gdown --id 1pOgV1mLyHkkfAIX39iqddG8mGBssN9jY\n",
        "#!gdown --id 1CmN8GPalrb45YFIPrWgh7GRYyoUhnizEOImY6kAW82w/edit#gid=396865486\n",
        "\n",
        "if (UseMigaku):\n",
        "  !gdown --id 151b1fyxMtodhsy2EIgQPUjEo3QyVXoCC\n",
        "\n",
        "#ParserPath = '' #@param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "JDuCj10Nmiv8",
        "outputId": "48dfc4a2-098b-4c6c-9dbe-cc16bcfa8c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WF2FkidAVMLKjdtOf0lWb21d2mYa7-06\n",
            "To: /content/parser.csv\n",
            "100% 415k/415k [00:00<00:00, 76.9MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PWglfvAwhN9CDBjJiqPt_DkZr132bFAE\n",
            "To: /content/freq.json\n",
            "100% 361k/361k [00:00<00:00, 24.3MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pOgV1mLyHkkfAIX39iqddG8mGBssN9jY\n",
            "To: /content/CantoneseVideosSheet.csv\n",
            "100% 3.78M/3.78M [00:00<00:00, 176MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=151b1fyxMtodhsy2EIgQPUjEo3QyVXoCC\n",
            "To: /content/learning.json\n",
            "100% 157k/157k [00:00<00:00, 79.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "videos_path = \"/content/CantoneseVideosSheet.csv\"\n",
        "with open(videos_path, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "newData = []\n",
        "for i in range(1,len(data)):\n",
        "  if(data[i][8] == 'Y'):\n",
        "    newData.append(data[i])\n",
        "\n",
        "#@title Run this code to complete set-up\n",
        "\n",
        "import csv\n",
        "\n",
        "### Import the Parsing Data ###\n",
        "\n",
        "parser_path = \"/content/parser.csv\"\n",
        "with open(parser_path, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "# We convert to a set for fast-lookup time\n",
        "data_list = []\n",
        "for i in data:\n",
        "  data_list.append(i[0])\n",
        "setParser = set(data_list)\n",
        "\n",
        "\n",
        "### Define our Longest-Substring-Match Parser ###\n",
        "# This uses a simple double-loop structure. Takes a long time\n",
        "# for longer videos but is usually fast-enough and\n",
        "# is very simple.\n",
        "def parseInput(inputString):\n",
        "  parsedOutput = []\n",
        "  i = 0\n",
        "  while (i<len(inputString)):\n",
        "    longestSub = i+1\n",
        "    for ii in range(i+1,i+21):\n",
        "      if (inputString[i:ii] in setParser):\n",
        "        longestSub = ii\n",
        "    parsedOutput.append(inputString[i:longestSub])\n",
        "    i = longestSub\n",
        "  return parsedOutput\n",
        "\n",
        "\n",
        "### Import the Frequency Data ###\n",
        "\n",
        "import json\n",
        "\n",
        "freq_path = \"/content/freq.json\"\n",
        "\n",
        "# Opening JSON file\n",
        "with open(freq_path) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    numList = list(range(len(data)))\n",
        "    freq_dict = dict(zip(data, numList))\n",
        "\n",
        "\n",
        "### OPTIONAL: Import Migaku Learning Status ###\n",
        "# We use a dictionary here for fast-lookup\n",
        "\n",
        "words_path = \"/content/learning.json\"\n",
        "\n",
        "with open(words_path) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    wordList = []\n",
        "    knownList = []\n",
        "    for i in range(len(data)):\n",
        "      word = data[i][0]\n",
        "      wordOnly = word.split(\"◴\")[0]\n",
        "      wordList.append(wordOnly)\n",
        "      knownList.append(data[i][1])\n",
        "    word_dict = dict(zip(wordList, knownList))\n",
        "\n",
        "!pip install youtube-transcript-api\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "print(\"Success\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "3gL46_QfSy9J",
        "outputId": "8f7ad37b-7297-45e2-f780-a4fbe9aba69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.4.4-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from youtube-transcript-api) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->youtube-transcript-api) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->youtube-transcript-api) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->youtube-transcript-api) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->youtube-transcript-api) (2.10)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.4.4\n",
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this code to generate results\n",
        "import tqdm\n",
        "from tqdm import trange\n",
        "\n",
        "import random\n",
        "indicies = random.sample(range(len(newData)), SampleSize)\n",
        "\n",
        "allGlobalFreqs = []\n",
        "allUnknownLearningFreqs = []\n",
        "allKnownPercentages = []\n",
        "allLearingPercentages = []\n",
        "allUnknownPercentages = []\n",
        "\n",
        "for i in trange(len(indicies)):\n",
        "\n",
        "  newIndex = indicies[i]\n",
        "  video_id = newData[newIndex][2]\n",
        "  codecs = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "\n",
        "  alltext = \"\"\n",
        "  for codec in codecs:\n",
        "    srt = YouTubeTranscriptApi.get_transcript(video_id, languages=[codec.language_code])\n",
        "    alltext = \"\"\n",
        "    for iii in range(len(srt)):\n",
        "      alltext += srt[iii]['text']\n",
        "    if (('係' in alltext) or ('佢' in alltext)):\n",
        "      finalCodec = codec\n",
        "\n",
        "  # Parse text into tokens\n",
        "  parsed = parseInput(alltext)\n",
        "\n",
        "  # Counters to track metrics\n",
        "  knownTokensNum = 0\n",
        "  learningTokensNum = 0\n",
        "  unknownTokensNum = 0\n",
        "  totalFreqSum = 0\n",
        "  unknownAndSeenFreqSum = 0\n",
        "\n",
        "  # Filter only those tokens which are in our parser\n",
        "  parseFiltered = []\n",
        "  for ii in range(len(parsed)):\n",
        "    if (parsed[ii] in setParser):\n",
        "      parseFiltered.append(parsed[ii])\n",
        "\n",
        "  maxFreq = len(freq_dict)\n",
        "  for ii in range(len(parseFiltered)):\n",
        "\n",
        "    # Total Frequency metrics\n",
        "    word = parseFiltered[ii]\n",
        "    freq = freq_dict.get(word, maxFreq)\n",
        "    totalFreqSum += freq\n",
        "\n",
        "    # Word learning status metrics\n",
        "    knownStatus = word_dict.get(word, -1)\n",
        "    if (knownStatus == 1):\n",
        "      learningTokensNum+=1\n",
        "    elif (knownStatus == 2):\n",
        "      knownTokensNum+=1\n",
        "    else:\n",
        "      unknownTokensNum+=1\n",
        "\n",
        "    # Learning status frequency metrics\n",
        "    if ((knownStatus == 1) or (knownStatus == -1)):\n",
        "      freq = freq_dict.get(word, maxFreq)\n",
        "      unknownAndSeenFreqSum += freq\n",
        "\n",
        "  totalTokens = learningTokensNum+knownTokensNum+unknownTokensNum\n",
        "  ulNum = learningTokensNum+unknownTokensNum\n",
        "\n",
        "  avgWordFreq = round(totalFreqSum / (len(parseFiltered)), 3)\n",
        "  avgUnknownLearningWordFreq = round(unknownAndSeenFreqSum/ulNum, 3)\n",
        "  percentKnown = round(knownTokensNum/totalTokens, 3)\n",
        "  percentLearning = round(learningTokensNum/totalTokens, 3)\n",
        "  percentUnknown = round(unknownTokensNum/totalTokens, 3)\n",
        "\n",
        "  allGlobalFreqs.append(avgWordFreq)\n",
        "  allUnknownLearningFreqs.append(avgUnknownLearningWordFreq)\n",
        "  allKnownPercentages.append(percentKnown)\n",
        "  allLearingPercentages.append(percentLearning)\n",
        "  allUnknownPercentages.append(percentUnknown)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "57KFQ5qBU3Co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00d3a18-4bb7-44db-b346-fcd9c20491db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.37s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this code to display and sort results\n",
        "SortBy = 'Known Word Percentage [Migaku Only]' #@param [\"Global Word Frequency\", \"Learning/Unknown Word Frequency [Migaku Only]\", \"Known Word Percentage [Migaku Only]\"]\n",
        "\n",
        "# Table package for nice prints\n",
        "!pip install tabulate\n",
        "import tabulate\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Clumsy way to sort based on the metric\n",
        "if (SortBy == \"Global Word Frequency\"):\n",
        "  allGlobalFreqs2, allKnownLearningFreqs2, allKnownPercentages2, allLearningPercentages2, allUnknownPercentages2, indicies2 = zip(*sorted(zip(allGlobalFreqs, allUnknownLearningFreqs, allKnownPercentages, allLearingPercentages, allUnknownPercentages, indicies)))\n",
        "elif (SortBy == \"Learning/Unknown Word Frequency [Migaku Only]\"):\n",
        "  assert UseMigaku==True, \"ERROR: You set UseMigaku as False. Either change to true in Option B or else change the sorting criteria\"\n",
        "  allKnownLearningFreqs2, allGlobalFreqs2, allKnownPercentages2, allLearningPercentages2, allUnknownPercentages2, indicies2 = zip(*sorted(zip(allUnknownLearningFreqs, allGlobalFreqs, allKnownPercentages, allLearingPercentages, allUnknownPercentages, indicies)))\n",
        "else:\n",
        "  assert UseMigaku==True, \"ERROR: You set UseMigaku as False. Either change to true in Option B or else change the sorting criteria\"\n",
        "  allKnownPercentages2, allGlobalFreqs2, allKnownLearningFreqs2, allLearningPercentages2, allUnknownPercentages2, indicies2 = zip(*sorted(zip(allKnownPercentages, allGlobalFreqs, allUnknownLearningFreqs, allLearingPercentages, allUnknownPercentages, indicies),reverse=True))\n",
        "\n",
        "# Clumsy way to write data to tables\n",
        "print(\"Index  URL AvgFreq\")\n",
        "newData2 = []\n",
        "for i in range(len(indicies2)):\n",
        "  newIndex = indicies2[i]\n",
        "  if UseMigaku:\n",
        "    newData2.append([i, \"https://youtube.com/watch?v=\" + newData[newIndex][2], allGlobalFreqs2[i], allKnownLearningFreqs2[i], allKnownPercentages2[i], allLearningPercentages2[i], allUnknownPercentages2[i]])\n",
        "  else:\n",
        "    newData2.append([i, \"https://youtube.com/watch?v=\" + newData[newIndex][2], allGlobalFreqs2[i]])\n",
        "\n",
        "# Clumsy way to assign headers lol\n",
        "if UseMigaku:\n",
        "  header = [\"Index\", \"URL\", \"Global Frequency\", \"Average UL Frequency\", \"KPercentage\", \"LPercentage\", \"UPercentage\"]\n",
        "else:\n",
        "  header = [\"Index\", \"URL\", \"Global Frequency\"]\n",
        "\n",
        "print(tabulate(newData2, headers=header))"
      ],
      "metadata": {
        "id": "L_9Bvpj-fW5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "62fd1877-0c57-4996-a32b-41e08d624601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.10)\n",
            "Index  URL AvgFreq\n",
            "  Index  URL                                        Global Frequency    Average UL Frequency    KPercentage    LPercentage    UPercentage\n",
            "-------  ---------------------------------------  ------------------  ----------------------  -------------  -------------  -------------\n",
            "      0  https://youtube.com/watch?v=IjgP9DjaXFU             1376.25                 6289.08          0.847          0.109          0.045\n",
            "      1  https://youtube.com/watch?v=hGHQHx_l2zA             1789.23                 7253.29          0.788          0.118          0.094\n",
            "      2  https://youtube.com/watch?v=UxIXbvlHsFI             2885.96                 8413.73          0.725          0.151          0.124\n",
            "      3  https://youtube.com/watch?v=KjTOg0DoRBA             2316.36                 5748.55          0.646          0.21           0.143\n",
            "      4  https://youtube.com/watch?v=OpjvRMyoAAs             6040.03                15459.9           0.638          0              0.362\n",
            "      5  https://youtube.com/watch?v=LKH1BxIVbz8             2007.5                  4860.83          0.631          0.235          0.134\n",
            "      6  https://youtube.com/watch?v=aNdNi09129E             2103.47                 4968.83          0.619          0.194          0.187\n",
            "      7  https://youtube.com/watch?v=-WiEe_afklQ             2441.25                 5606.51          0.618          0.189          0.193\n",
            "      8  https://youtube.com/watch?v=3E2E8yGViNE             2719.09                 5926.39          0.58           0.236          0.184\n",
            "      9  https://youtube.com/watch?v=E7Gdp_VJbp0             4108.61                 9074.85          0.575          0.251          0.174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ\n",
        "\n",
        "**How do I use my own Parser or Frequency list?**\n",
        "\n",
        "This is not too hard I hope. First, run the first cell of Option A or Option B depending on what you are parsing. This will import the default parsing and  frequency files (and optional Migaku Learning List file). Take a note of the format of the frequency and parser files. Frequency should be fine as-is if you use the Migaku format, but you will have to do some simple reformatting to make your parser file fit with the Colab since we expect csv format with just the word itself. I suggest using a simple replace macro that only copies each line up to the space character. Then, just rename to a CSV file and upload and overwrite these files with your own. I will probably try and get this to work with the native u8 file in the future but this was a lot easier.\n",
        "\n",
        "**Help! I still don't know how to import my own Migaku Data!**\n",
        "\n",
        "No problem! Here are step-by-step careful instructions.\n",
        "\n",
        "1.   Go to your Migaku Browser Extension settings and set to your language.\n",
        "2.   Go to the learning status Tab\n",
        "3.   Go to \"Export Backup\"\n",
        "4.   Notice the JSON file download. Rename to learning.json.\n",
        "5.   Run the first block of either Option A or Option B to import the other files. Click the Folder icon on the right side of the screen. It will say \"Files\". Notice that csv and json files. There should be one called \"learning.json\"/\n",
        "6.   Now just right click and click upload to upload and overwrite with your own learning data. Then run the rest of the code blocks and you are fine!\n",
        "\n",
        "NOTE: Everytime you run the first cell of either Option A or B, it will overwrite all the files once again and you have to upload your own files again. Sorry!\n",
        "\n",
        "**I got a strange error message (429) involving YouTube**\n",
        "\n",
        "This happens when you make too many API requests to YouTube. Try turning down your Sample Size and then waiting for a while before trying again.\n",
        "\n",
        "**I have further questions or don't understand something**\n",
        "\n",
        "Feel free to contact me on Discord (TofuChris) or else via email (chrisjwest99@gmail.com).\n",
        "\n",
        "Please enjoy!\n",
        "\n"
      ],
      "metadata": {
        "id": "zORCCZtnKxul"
      }
    }
  ]
}